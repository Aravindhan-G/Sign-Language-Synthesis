{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text To Speech\n",
    "The text is fed to gTTS library to get a .mp3 file which is then converted into .wav file. This is then played by pygame.mixer() and file is finally removed using an os command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gtts import gTTS\n",
    "from pygame import mixer\n",
    "from pydub import AudioSegment\n",
    "\n",
    "myobj = gTTS(text=mytext, lang='en', s\n",
    "myobj.save(\"sign_lang.mp3\")\n",
    "sound = AudioSegment.from_mp3(\"sign_lang.mp3\")\n",
    "sound.export(\"sign_lang.wav\", format=\"wav\")\n",
    "\n",
    "mixer.init()\n",
    "mixer.music.load('sign_lang.wav')\n",
    "mixer.music.play()\n",
    "os.system(\"rm sign_lang.wav sign_lang.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MediaPipe Hands\n",
    "We used hand tracking module of MediaPipe Framework to extract 21 hand landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for coords in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, coords, mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                    drawing_styles.get_default_hand_landmark_style(),\n",
    "                    drawing_styles.get_default_hand_connection_style())\n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Validation Split\n",
    "Split the data into train & validation data of specified ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "path = 'Downloads/ASL2/'\n",
    "train, valid = path+'train/', path+'valid/'\n",
    "if not os.path.exists(valid):\n",
    "    dirs = os.listdir(train)\n",
    "    os.system(f\"mkdir {valid}\")\n",
    "\n",
    "    for d in dirs:\n",
    "        os.system(f\"mkdir {valid+d}\")\n",
    "\n",
    "    for d in dirs:\n",
    "        sfol, dfol1 = train+d+'/', valid+d+'/'\n",
    "        trn = os.listdir(sfol)\n",
    "        val = random.sample(trn,350) \n",
    "        trn = [t for t in trn if t not in val]\n",
    "        print(\"Moving Files...\")\n",
    "        for x in val: os.system(f\"mv {sfol+x} {dfol1}\")\n",
    "        print(\"Done !\")\n",
    "\n",
    "else:\n",
    "    print(\"Folders created already....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Hand Coordinates\n",
    "Extracting 21 hand coordinates for our training images using existing MediaPipe model and storing it in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from PIL import Image\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.65)\n",
    "\n",
    "def extract_labels(img):     #image_path\n",
    "    image = cv2.imread(img)\n",
    "    #image = cv2.resize(image, (640,480),interpolation=cv2.INTER_CUBIC)\n",
    "    image = cv2.cvtColor(cv2.flip(image,1), cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process()\n",
    "    lst = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for coords in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, coords, mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                drawing_styles.get_default_hand_landmark_style(),\n",
    "                drawing_styles.get_default_hand_connection_style())\n",
    "        for crd in coords.landmark:\n",
    "            lst.append(crd.x)\n",
    "            lst.append(crd.y)\n",
    "            lst.append(crd.z)\n",
    "    else:\n",
    "        return None\n",
    "    return lst\n",
    "\n",
    "fldr = 'valid'  #'train'\n",
    "csvf, mode = 'test.csv', 'w'  #'train.csv', 'w'\n",
    "\n",
    "path = 'Downloads/ASL2/'+f'{fldr}/'\n",
    "labels = os.listdir(path)\n",
    "if not os.path.exists(csvf):\n",
    "    file = open(csvf, mode)\n",
    "    for lab in labels:\n",
    "        fol = path+lab+'/'\n",
    "        trn = os.listdir(fol)\n",
    "        print(lab)\n",
    "        for img in trn:\n",
    "            coords = extract_labels(fol+img)\n",
    "            if coords:\n",
    "                coords.append(lab)\n",
    "                file.write(','.join(map(str,coords))+'\\n')\n",
    "    print('Job Done...!')\n",
    "    file.close()\n",
    "\n",
    "else:\n",
    "    print('Data already extracted....')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our Model\n",
    "Training a ML model with our coordinate data file and saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier as XGB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('train.csv',header=None)\n",
    "le = LabelEncoder()\n",
    "le.fit(df[63])\n",
    "df[63] = le.transform(df[63])\n",
    "X, Y = np.asarray(df.iloc[:,:63]), np.asarray(df[63])\n",
    "\n",
    "xgb = XGB()\n",
    "xgb.fit(X,Y)\n",
    "\n",
    "pickle.dump(xgb, open('XGBoost.sav', 'wb'))\n",
    "pickle.dump(le, open('XEncoder1.sav','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Model\n",
    "Testing our model with test data and calculating model accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = pickle.load(open('XGBoost.sav','rb'))\n",
    "le = pickle.load(open('XEncoder1.sav','rb'))\n",
    "\n",
    "df2 = pd.read_csv('test_65.csv',header=None)\n",
    "tst_X, tst_Y = np.asarray(df2.iloc[:,:63]), np.asarray(df2[63])\n",
    "yhat = model.predict(np.array(tst_X))\n",
    "yhat = le.inverse_transform(yhat)\n",
    "score = accuracy_score(tst_Y, yhat)\n",
    "print(f\"Score : {score*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Workshop\n",
    "Here we combine our modules and our pre-trained ML Models to perform Sign Language To Speech process.\n",
    "Few optimization processes are ongoing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gtts import gTTS\n",
    "import mediapipe as mp\n",
    "from pygame import mixer\n",
    "from pydub import AudioSegment\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def play_voice(mytext):\n",
    "    myobj = gTTS(text=mytext, lang='en', slow=False)\n",
    "    myobj.save(\"sign_lang.mp3\")\n",
    "    sound = AudioSegment.from_mp3(\"sign_lang.mp3\")\n",
    "    sound.export(\"sign_lang.wav\", format=\"wav\")\n",
    "\n",
    "    mixer.init()\n",
    "    mixer.music.load('sign_lang.wav')\n",
    "    mixer.music.play()\n",
    "    os.system(\"rm sign_lang.wav sign_lang.mp3\")\n",
    "\n",
    "mod = pickle.load(open('XGBoost.sav', 'rb'))\n",
    "le = pickle.load(open('XEncoder1.sav', 'rb'))\n",
    "\n",
    "txt = ''\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev = time.time()\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        curr = time.time()\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if (curr-prev) >= 2:\n",
    "            prev = time.time()\n",
    "            if results.multi_hand_landmarks:\n",
    "                lst = []\n",
    "                for coords in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image, coords, mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                        drawing_styles.get_default_hand_landmarks_style(),\n",
    "                        drawing_styles.get_default_hand_connections_style())\n",
    "                for crd in coords.landmark:\n",
    "                    lst.append(crd.x)\n",
    "                    lst.append(crd.y)\n",
    "                    lst.append(crd.z)\n",
    "                lst = np.asarray(lst).reshape(1,-1)\n",
    "                tmp = mod.predict(lst)\n",
    "                txt += str(le.inverse_transform(tmp)).strip(\"'][\")\n",
    "                #play_voice(str(txt))\n",
    "        print(txt)\n",
    "        #cv2.putText(image, str(txt), (25, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,), 2, cv2.LINE_4)\n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-b7300b0e6367>, line 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b7300b0e6367>\"\u001b[0;36m, line \u001b[0;32m62\u001b[0m\n\u001b[0;31m    play_voice(str(txt))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gtts import gTTS\n",
    "import mediapipe as mp\n",
    "from pygame import mixer\n",
    "from pydub import AudioSegment\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def play_voice(mytext):\n",
    "    myobj = gTTS(text=mytext, lang='en', slow=False)\n",
    "    myobj.save(\"sign_lang.mp3\")\n",
    "    sound = AudioSegment.from_mp3(\"sign_lang.mp3\")\n",
    "    sound.export(\"sign_lang.wav\", format=\"wav\")\n",
    "\n",
    "    mixer.init()\n",
    "    mixer.music.load('sign_lang.wav')\n",
    "    mixer.music.play()\n",
    "    os.system(\"rm sign_lang.wav sign_lang.mp3\")\n",
    "\n",
    "mod = pickle.load(open('XGBoost.sav', 'rb'))\n",
    "le = pickle.load(open('XEncoder1.sav', 'rb'))\n",
    "\n",
    "txt = ''\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev = time.time()\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        curr = time.time()\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if (curr-prev) >= 2:\n",
    "            prev = time.time()\n",
    "            if results.multi_hand_landmarks:\n",
    "                lst = []\n",
    "                for coords in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image, coords, mp.solutions.hands.HAND_CONNECTIONS,\n",
    "                        drawing_styles.get_default_hand_landmark_style(),\n",
    "                        drawing_styles.get_default_hand_connection_style())\n",
    "                for crd in coords.landmark:\n",
    "                    lst.append(crd.x)\n",
    "                    lst.append(crd.y)\n",
    "                    lst.append(crd.z)\n",
    "                lst = np.asarray(lst).reshape(1,-1)\n",
    "                tmp = mod.predict(lst)\n",
    "                txt += str(le.inverse_transform(tmp)).strip(\"'][\"\n",
    "                play_voice(str(txt))\n",
    "        cv2.putText(image, str(txt), (25, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,), 2, cv2.LINE_4)\n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'model_char.sav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
